<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection App</title>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }
        #video {
            width: 640px;
            height: 480px;
            margin-bottom: 10px;
            border: 2px solid black;
        }
        button {
            padding: 10px 20px;
            margin: 10px;
            background-color: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
        }
        button:disabled {
            background-color: grey;
        }
    </style>
</head>
<body>
    <video id="video" autoplay></video>
    <div>
        <button id="startBtn">Start Detection</button>
        <button id="stopBtn" disabled>Stop Detection</button>
    </div>

    <script>
        const video = document.getElementById("video");
        const startBtn = document.getElementById("startBtn");
        const stopBtn = document.getElementById("stopBtn");

        let isDetecting = false;
        let alertSound = new Audio('https://github.com/Maxey1950/wakeupmom/raw/refs/heads/main/alert.mp3');

        // Initialize the camera
        navigator.mediaDevices.getUserMedia({ video: {} })
            .then(stream => {
                video.srcObject = stream;
                video.play();
            }).catch(err => {
                console.error("Error accessing camera: ", err);
            });

        // Load face-api.js models
        Promise.all([
            faceapi.nets.ssdMobilenetv1.loadFromUri("/models"),
            faceapi.nets.faceLandmark68Net.loadFromUri("/models"),
            faceapi.nets.faceRecognitionNet.loadFromUri("/models")
        ]).then(() => {
            console.log("Models loaded");

            // Start face detection when the "Start Detection" button is clicked
            startBtn.addEventListener("click", () => {
                isDetecting = true;
                startBtn.disabled = true;
                stopBtn.disabled = false;
                detectFace();
            });

            // Stop face detection when the "Stop Detection" button is clicked
            stopBtn.addEventListener("click", () => {
                isDetecting = false;
                startBtn.disabled = false;
                stopBtn.disabled = true;
            });

            // Function to detect faces and check if the person might be falling asleep
            async function detectFace() {
                if (isDetecting) {
                    const detections = await faceapi.detectAllFaces(video)
                        .withFaceLandmarks()
                        .withFaceDescriptors();
                    
                    // Check for closed eyes (as a simple indicator of sleepiness)
                    detections.forEach(detection => {
                        const landmarks = detection.landmarks;
                        const leftEye = landmarks.getLeftEye();
                        const rightEye = landmarks.getRightEye();
                        
                        // Check if eyes are closed by measuring the eye distance (simplified approach)
                        const leftEyeClosed = isEyeClosed(leftEye);
                        const rightEyeClosed = isEyeClosed(rightEye);
                        
                        if (leftEyeClosed && rightEyeClosed) {
                            console.log("Person is falling asleep");
                            playWakeUpSound();
                        }

                    });

                    requestAnimationFrame(detectFace);  // Keep detecting
                }
            }

            // Helper function to check if an eye is closed
            function isEyeClosed(eyeLandmarks) {
                const eyeWidth = Math.abs(eyeLandmarks[0].x - eyeLandmarks[3].x);
                const eyeHeight = Math.abs(eyeLandmarks[1].y - eyeLandmarks[5].y);
                const ratio = eyeHeight / eyeWidth;
                
                return ratio > 0.25;  // If the ratio is above a threshold, consider the eye closed
            }

            // Function to play the wake-up sound
            function playWakeUpSound() {
                if (alertSound.paused) {
                    alertSound.play();
                }
            }
        }).catch(err => {
            console.error("Error loading models: ", err);
        });
    </script>
</body>
</html>
